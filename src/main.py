import os
import re
import hashlib
from datetime import datetime, timezone
from typing import Dict, List, Any

import feedparser
import requests
import yaml
from dateutil import parser as dateparser

# â˜… è¿½åŠ ï¼šDB / Analytics / Dashboard
from src.db import get_conn, init_db, upsert_articles
from src.analytics import (
    query_daily_counts,
    write_daily_counts,
    compute_keywords,
    write_keywords,
)
from src.dashboard import write_dashboard_html


# ----------------------------
# Utility
# ----------------------------
def load_config(path: str) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        return yaml.safe_load(f)


def normalize_whitespace(s: str) -> str:
    return re.sub(r"\s+", " ", s).strip()


def parse_date(entry: dict) -> str:
    raw = entry.get("published") or entry.get("updated") or ""
    if not raw:
        return ""
    try:
        dt = dateparser.parse(raw)
        if not dt:
            return ""
        return dt.astimezone(timezone.utc).strftime("%Y-%m-%d")
    except Exception:
        return ""


def tag_category(title: str, keywords: Dict[str, List[str]]) -> str:
    t = title.lower()
    for cat, words in keywords.items():
        for w in words:
            if w.lower() in t:
                return cat
    return "ãã®ä»–"


def make_id_from_url(url: str) -> str:
    return hashlib.sha256(url.encode("utf-8")).hexdigest()


# ----------------------------
# Industry relevance filter
# ----------------------------
def is_relevant_to_industry(
    item: Dict[str, str],
    industry_keywords: List[str]
) -> bool:
    text = (item["title"] + " " + item["url"]).lower()
    return any(k.lower() in text for k in industry_keywords)


# ----------------------------
# Fetch (RSS)
# ----------------------------
def fetch_rss_items(
    sources: List[Dict[str, str]],
    keywords: Dict[str, List[str]],
    per_source_limit: int = 30
) -> List[Dict[str, str]]:
    items: List[Dict[str, str]] = []
    for src in sources:
        name = src["name"]
        rss_url = src["rss"]

        feed = feedparser.parse(rss_url)

        for e in feed.entries[:per_source_limit]:
            title = normalize_whitespace(e.get("title", ""))
            link = (e.get("link") or "").strip()
            if not title or not link:
                continue

            date = parse_date(e)
            category = tag_category(title, keywords)

            items.append({
                "date": date,
                "source": name,
                "title": title,
                "url": link,
                "category": category,
            })
    return items


def dedupe_by_url(items: List[Dict[str, str]]) -> List[Dict[str, str]]:
    seen = set()
    out = []
    for it in items:
        url = it["url"]
        if url in seen:
            continue
        seen.add(url)
        out.append(it)
    return out


def pick_top(items: List[Dict[str, str]], n: int) -> List[Dict[str, str]]:
    def score(it):
        return (1 if it["date"] else 0, it["date"])
    return sorted(items, key=score, reverse=True)[:n]


# ----------------------------
# Discord (SAFE)
# ----------------------------
DISCORD_LIMIT = 2000


def split_discord_message(text: str, limit: int = DISCORD_LIMIT) -> List[str]:
    chunks = []
    while len(text) > limit:
        split_at = text.rfind("\n", 0, limit)
        if split_at == -1:
            split_at = limit
        chunks.append(text[:split_at].strip())
        text = text[split_at:].lstrip()
    if text:
        chunks.append(text)
    return chunks


def post_to_discord_safe(webhook_url: str, content: str) -> None:
    messages = split_discord_message(content)
    for msg in messages:
        r = requests.post(webhook_url, json={"content": msg}, timeout=30)
        if r.status_code >= 300:
            raise RuntimeError(
                f"Discord webhook failed: {r.status_code} {r.text}"
            )


# ----------------------------
# OpenAI (Japanese brief)
# ----------------------------
def build_ai_input(items: List[Dict[str, str]]) -> str:
    lines = []
    for i, it in enumerate(items, start=1):
        lines.append(
            f"{i}. {it['title']} | å‡ºå…¸:{it['source']} | æ—¥ä»˜:{it.get('date','')} | ã‚«ãƒ†ã‚´ãƒª:{it.get('category','')} | {it['url']}"
        )
    return "\n".join(lines)


def summarize_with_openai_jp(industry: str, items: List[Dict[str, str]]) -> str:
    api_key = os.environ.get("OPENAI_API_KEY", "").strip()
    if not api_key:
        raise RuntimeError("Missing env var: OPENAI_API_KEY")

    model = os.environ.get("OPENAI_MODEL", "gpt-4.1-mini")
    input_text = build_ai_input(items[:10])

    url = "https://api.openai.com/v1/responses"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }

    today = datetime.now().strftime("%Y-%m-%d")

    prompt = f"""
ã‚ãªãŸã¯**ã€Œ{industry} å°‚é–€ãƒ¡ãƒ‡ã‚£ã‚¢ã€ã®ç·¨é›†è€…**ã§ã™ã€‚

ã€å³å®ˆãƒ«ãƒ¼ãƒ«ã€‘
- {industry} ã¨ç›´æ¥é–¢ä¿‚ã—ãªã„è©±é¡Œã¯**çµ¶å¯¾ã«å«ã‚ãªã„**
- åœ°æ”¿å­¦ãƒ»é‡‘èãƒ»æ”¿æ²»ã¯ã€Œæ¥­ç•Œã¨ç›´æ¥é–¢ä¿‚ã™ã‚‹å ´åˆã®ã¿ã€è¨€åŠå¯
- è¦‹å‡ºã—ã«å«ã¾ã‚Œã¦ã„ã¦ã‚‚ã€å†…å®¹ãŒæ¥­ç•Œå¤–ãªã‚‰é™¤å¤–
- æ¨æ¸¬ã§è©±é¡Œã‚’åºƒã’ãªã„
- äº‹å®Ÿã‚’æé€ ã—ãªã„

- æ—¥æœ¬èªã§å‡ºåŠ›
- å…¨ä½“ã¯1200ã€œ1800æ–‡å­—ä»¥å†…

å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆå¿…ãšã“ã®é †ï¼‰ï¼š
1) ã‚¿ã‚¤ãƒˆãƒ«ï¼š{industry} ãƒ‡ã‚¤ãƒªãƒ¼ãƒ–ãƒªãƒ¼ãƒ•ï¼ˆ{today}ï¼‰
2) ä»Šæ—¥ã®è¦ç‚¹ï¼ˆ3ã¤ï¼‰
3) ä»Šæ—¥ã®æ³¨ç›®ãƒ†ãƒ¼ãƒï¼ˆçŸ­ã„ä¸€è¨€ï¼‰
4) æ³¨ç›®ãƒ‹ãƒ¥ãƒ¼ã‚¹Top5ï¼š
   - è¦‹å‡ºã— / å‡ºå…¸ / ãªãœé‡è¦ã‹ï¼ˆ1è¡Œï¼‰ / URL
"""

    data = {
        "model": model,
        "input": [
            {"role": "system", "content": "You are a careful analyst. Do not invent facts."},
            {"role": "user", "content": prompt + "\n\nã€ç´ æã€‘\n" + input_text},
        ],
    }

    resp = requests.post(url, headers=headers, json=data, timeout=60)
    if resp.status_code >= 300:
        raise RuntimeError(f"OpenAI API failed: {resp.status_code} {resp.text}")

    j = resp.json()
    return j["output"][0]["content"][0]["text"].strip()


# ----------------------------
# Message builder (No AI fallback)
# ----------------------------
def build_basic_message(industry: str, top_items: List[Dict[str, str]], total_items: int) -> str:
    today = datetime.now().strftime("%Y-%m-%d")
    lines = []
    lines.append(f"ğŸ“Œ **{industry} ãƒ‹ãƒ¥ãƒ¼ã‚¹é€Ÿå ±**ï¼ˆ{today}ï¼‰")
    lines.append(f"å–å¾—ä»¶æ•°ï¼š{total_items}ï¼ˆAIè¦ç´„ãªã—ï¼‰")
    lines.append("")
    for i, it in enumerate(top_items, start=1):
        lines.append(f"{i}. {it['title']}")
        meta = f"å‡ºå…¸:{it['source']}"
        if it["date"]:
            meta += f" / æ—¥ä»˜:{it['date']}"
        meta += f" / ã‚«ãƒ†ã‚´ãƒª:{it['category']}"
        lines.append(meta)
        lines.append(it["url"])
        lines.append("")
    return "\n".join(lines).strip()


# ----------------------------
# Main
# ----------------------------
def main():
    config = load_config("src/config.yaml")

    industry = config.get("industry", "æ¥­ç•Œ")
    use_ai = bool(config.get("use_ai_summary", False))
    top_n = int(config.get("top_n", 5))

    keywords = config.get("keywords", {})
    sources = config.get("sources", [])

    if not sources:
        raise RuntimeError("No sources found in src/config.yaml")

    webhook_url = os.environ.get("DISCORD_WEBHOOK_URL", "").strip()
    if not webhook_url:
        raise RuntimeError("Missing env var: DISCORD_WEBHOOK_URL")

    # æ¥­ç•Œã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒ•ãƒ©ãƒƒãƒˆåŒ–
    industry_keywords: List[str] = []
    for words in keywords.values():
        industry_keywords.extend(words)

    # 1) fetch
    items = fetch_rss_items(sources, keywords, per_source_limit=30)

    # 2) normalize/dedupe
    items = dedupe_by_url(items)

    # 3) æ¥­ç•Œãƒ•ã‚£ãƒ«ã‚¿
    items = [it for it in items if is_relevant_to_industry(it, industry_keywords)]

    # ----------------------------
    # 4) SQLiteä¿å­˜ï¼ˆå±¥æ­´ï¼‰
    # ----------------------------
    conn = get_conn()
    init_db(conn)

    now_iso = datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ")
    rows = []
    for it in items:
        # â˜…å¤‰æ›´ç‚¹ï¼šdateãŒç©ºãªã‚‰ Noneï¼ˆ= SQLiteã§ã¯NULLï¼‰
        d = (it.get("date") or "").strip()
        rows.append({
            "id": make_id_from_url(it["url"]),
            "date": d if d else None,
            "source": it.get("source", ""),
            "title": it.get("title", ""),
            "url": it.get("url", ""),
            "category": it.get("category", ""),
            "created_at": now_iso,
        })

    inserted = upsert_articles(conn, rows)
    print(f"DB inserted: {inserted} new rows")

    # ----------------------------
    # 5) é›†è¨ˆ & ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æ›´æ–°
    # ----------------------------
    payload = query_daily_counts(conn, days=30)
    write_daily_counts(payload)
    write_dashboard_html()

    kw = compute_keywords(conn, days=30)
    write_keywords(kw)

    print("Dashboard & analytics updated")

    # ----------------------------
    # 6) Discordç”¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆ
    # ----------------------------
    top_items = pick_top(items, n=top_n)

    if use_ai:
        try:
            ai_items = pick_top(items, n=min(10, len(items)))
            message = summarize_with_openai_jp(industry, ai_items)
        except Exception as e:
            message = build_basic_message(industry, top_items, total_items=len(items))
            message += f"\n\nï¼ˆAIè¦ç´„ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ç°¡æ˜“ç‰ˆï¼‰\n{e}"
    else:
        message = build_basic_message(industry, top_items, total_items=len(items))

    dashboard_url = os.environ.get("DASHBOARD_URL", "").strip()
    if dashboard_url:
        message += f"\n\nğŸ“Š ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰: {dashboard_url}"

    # 7) post to Discord
    post_to_discord_safe(webhook_url, message)
    print("Posted to Discord successfully.")


if __name__ == "__main__":
    main()
